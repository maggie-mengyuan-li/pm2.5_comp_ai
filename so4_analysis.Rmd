---
title: "SO4 Analysis"
author: "Maggie Li (ml4424)"
date: "9/28/2021"
output: html_document
---

```{r load libraries}
library(tidyverse)
library(janitor)
library(stats)
library(lme4)
library(raster)
library(sf)
library(usmap)
library(tictoc)
library(ncdf4)
```


# Aim 1: Compare county-level SO4 concentrations in 2000-2017 between AI and non-AI-populated counties

```{r Read in file with county type defined for each 3109 study counties}
ai_county_fips = read_csv("data/ai_counties.csv") 

nai_county_fips = read_csv("data/nai_counties.csv") 
```

## Extract county-level SO4 concentrations for 2000-2017

Data Source: Randall Martin's PM2.5 Model https://sites.wustl.edu/acag/datasets/surface-pm2-5/ 
Note: The raw data show the proportion of each component, so have to multiply it by the total PM concentration that year.

For Vivian: The next 2 code chunks will take a while to run, especially looping through all years. If it's too much, you can run just one year or a few; the outputted datasets are already in the folders indicated by write_csv. 

Instead of running the code separately for the 48 states and DC (as was the case with BC, NO3, NH4, Soil, OM), I appended the DC FIPS code ("11") into the state_fips vector so I could extract them in the same loop 

```{r Prepare by reading in county shapefiles and organizing by state}
# read in all counties shapefile
counties_shp <- "data/cb_2018_us_county_500k/cb_2018_us_county_500k.shp"
all_counties <- st_read(counties_shp, stringsAsFactors = FALSE)

# group all_counties by state fips, exclude territories and hawaii and alaska
all_counties <- all_counties %>% arrange(STATEFP) %>%
  filter(STATEFP != "02" & STATEFP != "15" & STATEFP <= 56)
unique(all_counties$STATEFP)
all_counties

# each state's fips code in a vector
state_fips <- fips(state.name, county = c())
fips()

# remove hawaii and alaska (FIPS are 15 and 02)
state_fips <- state_fips[!(state_fips %in% c("02","15"))] %>%
  append("11") # Add in DC as a "state" fips
state_fips

# save each state's counties as separate sf's, in a list. the function below will iterate through all of these states.
state_list <- list(length(state_fips)) # create list of length 49
for (i in 1:length(state_fips)){
  state_list[[i]] <- all_counties %>%
    filter(STATEFP==state_fips[i])
}
# state_list # each list value = one state; includes polygon counties in that state; 48 states + DC = 49 "states" total
length(state_fips)

```

### Run loops to read in and get all SO4 data 2000-2017 in DC and 48 contiguous states

```{r Create vectors of the file names to read in through loop}
so4_years = c("so4_2000", "so4_2001", "so4_2002", "so4_2003", "so4_2004", "so4_2005",
             "so4_2006", "so4_2007", "so4_2008", "so4_2009",
             "so4_2010", "so4_2011", "so4_2012", "so4_2013",
             "so4_2014", "so4_2015", "so4_2016", "so4_2017")

pm_years = c("pm_2000", "pm_2001", "pm_2002", "pm_2003", "pm_2004", "pm_2005",
             "pm_2006", "pm_2007", "pm_2008", "pm_2009",
             "pm_2010", "pm_2011", "pm_2012", "pm_2013",
             "pm_2014", "pm_2015", "pm_2016", "pm_2017")
```

```{r Extract Data for 48 contiguous states + 1 DC}
# tic() # tictoc helps record the time it takes to run things in R
# 
# # count variable for year to name saved out dfs
# year = 2000
# 
# ## Run loop for 2000 through 2017
# for (i in 1:1){
#   # read in ncdf for so4 data downloaded from Randall's website, convert to raster stack 
#   so4_year = raster(paste("data/raw_components/so4/", so4_years[i], ".nc", sep = ""))
#   so4_year = stack(so4_year)
# 
# 
#   # read in ncdf as raster stack for total PM 
#   pm_year = raster(paste("data/raw_components/total_pm/", pm_years[i], ".nc", sep = ""))
#   pm_year = stack(pm_year)
# 
#   # get the actual raster matrix for BC through multiplying percentage with total PM
#   so4_actual_year = (so4_year*pm_year)/100
#   mean(getValues(so4_actual_year), na.rm = T) # average county mean
# 
#   # create empty lists to save dfs into; (each list item [[i]] = one state, each row = county within that state)
#   so4_year_list = list()
# 
#   # Nested loop to extract data for all 48 states +1 for DC for each year
#   for (j in 1:49){ 
#     so4_year_list[[j]] <- raster::extract(so4_actual_year,
#                               state_list[[j]],
#                               fun=mean, na.rm=TRUE, df=TRUE) #specify function = mean to extract mean concentrations
#     so4_year_list[[j]]$County <- state_list[[j]]$COUNTYFP
#     so4_year_list[[j]]$State <- state_fips[j]
#     so4_year_list[[j]]$FIPS <- paste(so4_year_list[[j]]$State,
#                          so4_year_list[[j]]$County)
#     so4_year_list[[j]]$FIPS <- str_replace_all(so4_year_list[[j]]$FIPS, " ", "")
#     } # end nested loop
# 
#   # PM25 for each county should be saved now in the previously empty list
#   # (each list item = one state, each row = county within that state)
#   so4_year_df = do.call(rbind, so4_year_list) # combine all list items into one df
#   write_csv(so4_year_df,
#             paste("data/county_concentrations/so4/so4_", year, "_df.csv", sep = ""))
#   year = year + 1 # year +1 so every iteration reads out the next year's data into file
# 
# }
# toc()
```

```{r combine all years into one df}
# # Read in model data into list of length 19 (# study years)
# so4_allyrs <- list()
# yr = 2000
# for (i in 1:18){
#   so4_allyrs[[i]] <- read_csv(file = paste('data/county_concentrations/so4/so4_',
#                                            yr, '_df.csv', sep = '')) %>%
#     dplyr::rename(so4 = layer) %>% # rename "layer" column to specify component (SO4)
#     mutate(year = yr) # add column for year
#   yr <- yr + 1
# }
# 
# so4_allyrs = do.call(rbind, so4_allyrs) # join list into one df
# 
# # UPDATE: join with DC data
# so4_allyrs = so4_allyrs %>% dplyr::select(-ID)
# so4_allyrs = rbind(so4_allyrs, so4_dc)
# 
# summary(so4_allyrs)
# length(unique(so4_allyrs$FIPS))
# 
# # write out Final joined dataset w/ 48 states + DC
# write_csv(so4_allyrs, "data/county_concentrations/allyrs/so4_allyrs.csv")

# Note: Final joined dataset w/ 48 states + DC saved in "data/county_concentrations/allyrs/" folder
# county_concentrations folder with components contains county-level concentrations without DC
```

## Assign SO4 estimates to AI vs non-AI counties

```{r Join layer to AI county assignment}
so4_allyrs = read_csv("data/county_concentrations/allyrs/so4_allyrs.csv")

all_county_types = rbind(ai_county_fips, nai_county_fips) %>% 
  rename(FIPS = County)

so4_ai_join = left_join(so4_allyrs, all_county_types) %>% 
  dplyr::select(FIPS, so4, year, county_type) %>% 
  rename(County = FIPS) %>% 
  mutate(county_type = replace_na(county_type, 1)) ## county FIPS 46102 is showing up as an NA
so4_ai_join
```

```{r Join with covariates}
covariates = read_csv("data/covariates.csv")
summary(covariates)
so4_ai_join = so4_ai_join %>% 
  left_join(covariates)
so4_ai_join

# Compare summaries in all counties, AI, non-AI
summary(so4_ai_join)
summary(so4_ai_join %>% filter(county_type == 1))
summary(so4_ai_join %>% filter(county_type == 0))

# note higher median/mean levels in nAI counties (unadjusted)
```

```{r Add columns for deciles}
# Split population density and hhincome into deciles for model
so4_ai_join$popd_q <- cut(so4_ai_join$pop_density, quantile(so4_ai_join$pop_density, seq(0,1,0.1)), include.lowest = TRUE)
so4_ai_join$hhinc_q <- cut(so4_ai_join$hh_income, quantile(so4_ai_join$hh_income, seq(0,1,0.1)), include.lowest = TRUE)
sum(table(so4_ai_join$popd_q, exclude = NULL)) == dim(so4_ai_join)[1]
so4_ai_join


```
### Statistical Models:

```{r Run lmer models}
  # set referent for df
so4_ai_join = so4_ai_join %>% 
  mutate(county_type = factor(county_type)) %>% 
  mutate(county_type = relevel(county_type, ref="0"))

## Save out dataframe 
write_csv(so4_ai_join,
          "data/county_concentrations/joined_dta/so4_ai_join.csv")

# unadjusted
so4_unadj = lmer(so4 ~ county_type + as.factor(year) +
                    (1|State/County),
                  data = so4_ai_join)

## 95% CI: point estimate, LL, UL
paste(summary(so4_unadj)$coefficients[2,1] %>% round(digits = 3),
      " (",
      round(summary(so4_unadj)$coefficients[2,1] - 1.96*summary(so4_unadj)$coefficients[2,2], digits = 3),
      ", ",
      round(summary(so4_unadj)$coefficients[2,1] + 1.96*summary(so4_unadj)$coefficients[2,2], digits = 3),
      ")", sep = "")

# additionally adjusted for popd
so4_partial = lmer(so4 ~ county_type + as.factor(year) +
                  popd_q +
                  (1|State/County),
                data = so4_ai_join)

## 95% CI: point estimate, LL, UL
paste(summary(so4_partial)$coefficients[2,1] %>% round(digits = 3),
      " (",
      round(summary(so4_partial)$coefficients[2,1] - 1.96*summary(so4_partial)$coefficients[2,2], digits = 3),
      ", ",
      round(summary(so4_partial)$coefficients[2,1] + 1.96*summary(so4_partial)$coefficients[2,2], digits = 3),
      ")", sep = "")

# full adj model
so4_full = lmer(so4 ~ county_type + as.factor(year) +
                    popd_q + 
                    hhinc_q +
                    (1|State/County),
                  data = so4_ai_join, REML=FALSE)

## 95% CI: point estimate, LL, UL
paste(summary(so4_full)$coefficients[2,1] %>% round(digits = 3),
      " (",
      round(summary(so4_full)$coefficients[2,1] - 1.96*summary(so4_full)$coefficients[2,2], digits = 3),
      ", ",
      round(summary(so4_full)$coefficients[2,1] + 1.96*summary(so4_full)$coefficients[2,2], digits = 3),
      ")", sep = "")

paste(summary(so4_full)$coefficients[2,1] %>% round(digits = 2),
      " (",
      round(summary(so4_full)$coefficients[2,1] - 1.96*summary(so4_full)$coefficients[2,2], digits = 2),
      ", ",
      round(summary(so4_full)$coefficients[2,1] + 1.96*summary(so4_full)$coefficients[2,2], digits = 2),
      ")", sep = "")

# fully adjusted with interaction term for year
so4_full_interx = lmer(so4 ~ county_type + as.factor(year) +
                    popd_q + 
                    hhinc_q +
                    county_type*as.factor(year) +
                    (1|State/County),
                  data = so4_ai_join, REML=FALSE)
summary(so4_full_interx)
```

```{r Save out results from main effects only model and interx effects model}

## save model output for main effects only model
saveRDS(so4_full,
        file = "intermediate/model_outputs/main_results/so4_main_effects.rds")
## save model output for main effects only model
saveRDS(so4_full_interx,
        file = "intermediate/model_outputs/main_results/so4_interx.rds")

```

```{r InterX plot from lmer}
# create vcov matrix of main effects and interX

# extract vcov matrix of the county type main effect and interaction effect for each categorical year
native_yr_vcov <- vcov(so4_full_interx)[c(2,seq(38,54)), c(2,seq(38,54))]

# calculate all the variances for all the years i.e. var(x+y); should be 19 total entries --> var_vector
# var_vector should be the same repeating value (from the diagonal of the vcov matrix above)
var_vector = c()
for (i in 2:18){
  var_vector[1] <- native_yr_vcov[1,1]
  var_vector[i] <- native_yr_vcov[1,1] + native_yr_vcov[i,i] + 2 * native_yr_vcov[i,1]
}
var_vector
sd_vector <- sqrt(var_vector)
length(sd_vector)
#matrix with 19 cols for 19 years, three rows: one for effect estimate of total effect per year (total effect = main effect + interx effect), one for CI lower, one for CI upper
so4_decline <- data.frame()
so4_decline[1,1] <- summary(so4_full_interx)$coefficients[2,1]
so4_decline[1,2] <- summary(so4_full_interx)$coefficients[2,1] - 1.96*sd_vector[1]
so4_decline[1,3] <- summary(so4_full_interx)$coefficients[2,1] + 1.96*sd_vector[1]

# fill in matrix thru loop for every following year
yr_ct <- 38
for (i in 2:18){
  so4_decline[i,1] <- summary(so4_full_interx)$coefficients[2,1]+
    summary(so4_full_interx)$coefficients[yr_ct,1]
  
  so4_decline[i,2] <- summary(so4_full_interx)$coefficients[2,1]+
    summary(so4_full_interx)$coefficients[yr_ct,1] - 1.96*sd_vector[i]
  
  so4_decline[i,3] <- summary(so4_full_interx)$coefficients[2,1]+
    summary(so4_full_interx)$coefficients[yr_ct,1] + 1.96*sd_vector[i]
  yr_ct <- yr_ct + 1
}

so4_decline

colnames(so4_decline) <- c('estimate', 'cl_lower', 'cl_upper') # set col names

so4_decline$year <- seq(2000, 2017) # column for year
```

```{r}
#PLOT OF TOTAL EFFECT OF NATIVE OVER TIME; updated 12/14 to be consistent with Figure 3 layout (slanted x-axis label)
so4_interx_plot <- ggplot() + 
  theme_linedraw() + 
  geom_line(data = so4_decline,
            aes(x=year, y = estimate)) +
  geom_line(data=so4_decline,
            aes(x=year, y=cl_lower), linetype = "dashed") +
  geom_line(data=so4_decline,
            aes(x=year, y=cl_upper), linetype = "dashed") +
  # ylim(-0.61, 0.6) +
  labs(x = "Year",
       y = expression(paste("Mean Difference in ", "SO"[4]^"2-", " (", mu, "g/", m^3, ")")),
       fill = "County Type") +
  theme(plot.title = element_text(size = 18),
        axis.title = element_text(size = 16),
        axis.text = element_text(size = 12),
        axis.title.x=element_blank(),
        panel.background = element_rect(fill='transparent'), 
        plot.background = element_rect(fill='transparent', color=NA), 
    ) +
  scale_x_continuous(breaks = seq(2000,2017,1), expand = c(0, 0)) +
  guides(x =  guide_axis(angle = 45)) +
  ylim(-0.62, 0.6) +
  geom_hline(yintercept=0, linetype="solid", color = "red")
so4_interx_plot
ggsave("figures/effectmod_plots/main/so4.png", width = 6, height = 4, dpi = 300,
       bg='transparent')

```

## Smooth interX model with time

To Vivian: Given similarities with the linear models, we did not choose to include them in our paper, opting instead for the GLMM results above. You can run the below code to check it out if you wish to.

```{r 10 knots}
# # using splines
# library(splines)
# library(mgcv)
# library(gamm4)
# so4_full_interx_smooth = gamm4(so4 ~ s(year, by = county_type, k = 10) + # specify k knots
#                     county_type+
#                       popd_q + 
#                     hhinc_q,
#                     random = ~(1|State/County),
#                   data =  so4_ai_join)
# summary(so4_full_interx_smooth)
# summary(so4_full_interx_smooth$gam)
# summary(so4_full_interx_smooth$mer)
# plot(so4_full_interx_smooth$gam, pages = 1)
# 
# 
# 
# # empty df for predicted values from gam
# pdat = expand.grid(year = seq(2000, 2017, length = 170),
#                     county_type = c(0, 1),
#                    popd_q = "(160,382]",
#                    hhinc_q = "(4.45e+04,4.72e+04]")
# 
# 
# # add in predicted values from gam
# xp <- predict(so4_full_interx_smooth$gam, newdata = pdat, type = 'lpmatrix')
# xp
# 
# ## which cols of xp relate to splines of interest?
# c1 <- grepl('0', colnames(xp))
# c2 <- grepl('1', colnames(xp))
# ## which rows of xp relate to sites of interest?
# r1 <- with(pdat, county_type == '0')
# r2 <- with(pdat, county_type == '1')
# 
# ## difference rows of xp for data from comparison
# X <- xp[r1, ] - xp[r2, ]
# ## zero out cols of X related to splines for other lochs
# X[, ! (c1 | c2)] <- 0
# ## zero out the parametric cols
# X[, !grepl('^s\\(', colnames(xp))] <- 0
# #extract betas
# dif <- X %*% coef(so4_full_interx_smooth$gam)
# #extract se
# se <- sqrt(rowSums((X %*% vcov(so4_full_interx_smooth$gam)) * X))
# #calculate CI
# crit <- qt(.975, df.residual(so4_full_interx_smooth$gam))
# upr <- dif + (crit * se)
# lwr <- dif - (crit * se)
# 
# # function to do it all in 1
# smooth_diff <- function(model, newdata, f1, f2, var, alpha = 0.05,
#                         unconditional = FALSE) {
#     xp <- predict(model, newdata = newdata, type = 'lpmatrix')
#     c1 <- grepl(f1, colnames(xp))
#     c2 <- grepl(f2, colnames(xp))
#     r1 <- newdata[[var]] == f1
#     r2 <- newdata[[var]] == f2
#     ## difference rows of xp for data from comparison
#     X <- xp[r1, ] - xp[r2, ]
#     ## zero out cols of X related to splines for other lochs
#     X[, ! (c1 | c2)] <- 0
#     ## zero out the parametric cols
#     X[, !grepl('^s\\(', colnames(xp))] <- 0
#     dif <- X %*% coef(model)
#     se <- sqrt(rowSums((X %*% vcov(model, unconditional = unconditional)) * X))
#     crit <- qt(alpha/2, df.residual(model), lower.tail = FALSE)
#     upr <- dif + (crit * se)
#     lwr <- dif - (crit * se)
#     data.frame(pair = paste(f1, f2, sep = '-'),
#                diff = dif,
#                se = se,
#                upper = upr,
#                lower = lwr)
# }
# 
# comp_so4 <- smooth_diff(so4_full_interx_smooth$gam, pdat, '1', '0', 'county_type')
# comp <- cbind(year = seq(2000, 2017, length = 170),
#               rbind(comp_so4))
# 
# comp
# 
# # plot smoothed difference
# ggplot(comp, aes(x = year, y = diff, group = pair)) +
#   theme_linedraw() +
#     geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
#     geom_line() +
#   # ylim(-0.61, 0.6) +
#   labs(x = "Year",
#        y = expression(paste("Mean Difference in SO4 (", mu, "g/", m^3, ")")),
#        fill = "County Type") +
#   theme(plot.title = element_text(size = 18),
#         axis.title = element_text(size = 16),
#         axis.text = element_text(size = 12),
#         axis.title.x=element_blank()) +
#   scale_x_continuous(breaks = seq(2000,2017,1), expand = c(0, 0)) +
#   guides(x =  guide_axis(angle = 45)) +
#   ylim(-0.6, 0.63) +
#   geom_hline(yintercept=0, linetype="solid", color = "red")
# 
# ggsave("figures/effectmod_plots/so4/so4_smooth_k10.png", width = 6, height = 4, dpi = 300)
# # compare with original glmm
# so4_interx_plot
```


```{r 7 knots}
# # using splines
# library(splines)
# library(mgcv)
# library(gamm4)
# so4_full_interx_smooth = gamm4(so4 ~ s(year, by = county_type, k = 7) + # specify k knots
#                     county_type+
#                       popd_q + 
#                     hhinc_q,
#                     random = ~(1|State/County),
#                   data =  so4_ai_join)
# summary(so4_full_interx_smooth)
# summary(so4_full_interx_smooth$gam)
# 
# plot(so4_full_interx_smooth$gam, pages = 1)
# 
# 
# 
# # empty df for predicted values from gam
# pdat = expand.grid(year = seq(2000, 2017, length = 170),
#                     county_type = c(0, 1),
#                    popd_q = "(160,382]",
#                    hhinc_q = "(4.45e+04,4.72e+04]")
# 
# 
# # add in predicted values from gam
# xp <- predict(so4_full_interx_smooth$gam, newdata = pdat, type = 'lpmatrix')
# xp
# 
# ## which cols of xp relate to splines of interest?
# c1 <- grepl('0', colnames(xp))
# c2 <- grepl('1', colnames(xp))
# ## which rows of xp relate to sites of interest?
# r1 <- with(pdat, county_type == '0')
# r2 <- with(pdat, county_type == '1')
# 
# ## difference rows of xp for data from comparison
# X <- xp[r1, ] - xp[r2, ]
# ## zero out cols of X related to splines for other lochs
# X[, ! (c1 | c2)] <- 0
# ## zero out the parametric cols
# X[, !grepl('^s\\(', colnames(xp))] <- 0
# #extract betas
# dif <- X %*% coef(so4_full_interx_smooth$gam)
# #extract se
# se <- sqrt(rowSums((X %*% vcov(so4_full_interx_smooth$gam)) * X))
# #calculate CI
# crit <- qt(.975, df.residual(so4_full_interx_smooth$gam))
# upr <- dif + (crit * se)
# lwr <- dif - (crit * se)
# 
# # function to do it all in 1
# smooth_diff <- function(model, newdata, f1, f2, var, alpha = 0.05,
#                         unconditional = FALSE) {
#     xp <- predict(model, newdata = newdata, type = 'lpmatrix')
#     c1 <- grepl(f1, colnames(xp))
#     c2 <- grepl(f2, colnames(xp))
#     r1 <- newdata[[var]] == f1
#     r2 <- newdata[[var]] == f2
#     ## difference rows of xp for data from comparison
#     X <- xp[r1, ] - xp[r2, ]
#     ## zero out cols of X related to splines for other lochs
#     X[, ! (c1 | c2)] <- 0
#     ## zero out the parametric cols
#     X[, !grepl('^s\\(', colnames(xp))] <- 0
#     dif <- X %*% coef(model)
#     se <- sqrt(rowSums((X %*% vcov(model, unconditional = unconditional)) * X))
#     crit <- qt(alpha/2, df.residual(model), lower.tail = FALSE)
#     upr <- dif + (crit * se)
#     lwr <- dif - (crit * se)
#     data.frame(pair = paste(f1, f2, sep = '-'),
#                diff = dif,
#                se = se,
#                upper = upr,
#                lower = lwr)
# }
# 
# comp_so4 <- smooth_diff(so4_full_interx_smooth$gam, pdat, '1', '0', 'county_type')
# comp <- cbind(year = seq(2000, 2017, length = 170),
#               rbind(comp_so4))
# 
# comp
# 
# # plot smoothed difference
# ggplot(comp, aes(x = year, y = diff, group = pair)) +
#   theme_linedraw() +
#     geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
#     geom_line() +
#   # ylim(-0.61, 0.6) +
#   labs(x = "Year",
#        y = expression(paste("Mean Difference in SO4 (", mu, "g/", m^3, ")")),
#        fill = "County Type") +
#   theme(plot.title = element_text(size = 18),
#         axis.title = element_text(size = 16),
#         axis.text = element_text(size = 12),
#         axis.title.x=element_blank()) +
#   scale_x_continuous(breaks = seq(2000,2017,1), expand = c(0, 0)) +
#   guides(x =  guide_axis(angle = 45)) +
#   ylim(-0.6, 0.6) +
#   geom_hline(yintercept=0, linetype="solid", color = "red")
# 
# ggsave("figures/effectmod_plots/so4/so4_smooth_k7.png", width = 6, height = 4, dpi = 300)
# # compare with original glmm
# so4_interx_plot
```

```{r 5 knots}
# # using splines
# library(splines)
# library(mgcv)
# library(gamm4)
# so4_full_interx_smooth = gamm4(so4 ~ s(year, by = county_type, k = 5) + # specify k knots
#                     county_type+
#                       popd_q + 
#                     hhinc_q,
#                     random = ~(1|State/County),
#                   data =  so4_ai_join)
# summary(so4_full_interx_smooth)
# summary(so4_full_interx_smooth$gam)
# 
# plot(so4_full_interx_smooth$gam, pages = 1)
# 
# 
# 
# # empty df for predicted values from gam
# pdat = expand.grid(year = seq(2000, 2017, length = 170),
#                     county_type = c(0, 1),
#                    popd_q = "(160,382]",
#                    hhinc_q = "(4.45e+04,4.72e+04]")
# 
# 
# # add in predicted values from gam
# xp <- predict(so4_full_interx_smooth$gam, newdata = pdat, type = 'lpmatrix')
# xp
# 
# ## which cols of xp relate to splines of interest?
# c1 <- grepl('0', colnames(xp))
# c2 <- grepl('1', colnames(xp))
# ## which rows of xp relate to sites of interest?
# r1 <- with(pdat, county_type == '0')
# r2 <- with(pdat, county_type == '1')
# 
# ## difference rows of xp for data from comparison
# X <- xp[r1, ] - xp[r2, ]
# ## zero out cols of X related to splines for other lochs
# X[, ! (c1 | c2)] <- 0
# ## zero out the parametric cols
# X[, !grepl('^s\\(', colnames(xp))] <- 0
# #extract betas
# dif <- X %*% coef(so4_full_interx_smooth$gam)
# #extract se
# se <- sqrt(rowSums((X %*% vcov(so4_full_interx_smooth$gam)) * X))
# #calculate CI
# crit <- qt(.975, df.residual(so4_full_interx_smooth$gam))
# upr <- dif + (crit * se)
# lwr <- dif - (crit * se)
# 
# # function to do it all in 1
# smooth_diff <- function(model, newdata, f1, f2, var, alpha = 0.05,
#                         unconditional = FALSE) {
#     xp <- predict(model, newdata = newdata, type = 'lpmatrix')
#     c1 <- grepl(f1, colnames(xp))
#     c2 <- grepl(f2, colnames(xp))
#     r1 <- newdata[[var]] == f1
#     r2 <- newdata[[var]] == f2
#     ## difference rows of xp for data from comparison
#     X <- xp[r1, ] - xp[r2, ]
#     ## zero out cols of X related to splines for other lochs
#     X[, ! (c1 | c2)] <- 0
#     ## zero out the parametric cols
#     X[, !grepl('^s\\(', colnames(xp))] <- 0
#     dif <- X %*% coef(model)
#     se <- sqrt(rowSums((X %*% vcov(model, unconditional = unconditional)) * X))
#     crit <- qt(alpha/2, df.residual(model), lower.tail = FALSE)
#     upr <- dif + (crit * se)
#     lwr <- dif - (crit * se)
#     data.frame(pair = paste(f1, f2, sep = '-'),
#                diff = dif,
#                se = se,
#                upper = upr,
#                lower = lwr)
# }
# 
# comp_so4 <- smooth_diff(so4_full_interx_smooth$gam, pdat, '1', '0', 'county_type')
# comp <- cbind(year = seq(2000, 2017, length = 170),
#               rbind(comp_so4))
# 
# comp
# 
# # plot smoothed difference
# ggplot(comp, aes(x = year, y = diff, group = pair)) +
#   theme_linedraw() +
#     geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
#     geom_line() +
#   # ylim(-0.61, 0.6) +
#   labs(x = "Year",
#        y = expression(paste("Mean Difference in SO4 (", mu, "g/", m^3, ")")),
#        fill = "County Type") +
#   theme(plot.title = element_text(size = 18),
#         axis.title = element_text(size = 16),
#         axis.text = element_text(size = 12),
#         axis.title.x=element_blank()) +
#   scale_x_continuous(breaks = seq(2000,2017,1), expand = c(0, 0)) +
#   guides(x =  guide_axis(angle = 45)) +
#   ylim(-0.6, 0.6) +
#   geom_hline(yintercept=0, linetype="solid", color = "red")
# 
# ggsave("figures/effectmod_plots/so4/so4_smooth_k5.png", width = 6, height = 4, dpi = 300)
# # compare with original glmm
# so4_interx_plot
```

```{r testing example code for reading in SO4 from total PM paper}
#  # Extract Data for DC
# ## Note: we have to extract concentrations in DC separately,  because DC was not a state in the state_list sf file.
# ## create a df just DC all years, join with the existing components_year_df files in data/county_concentrations
# DC_state_list <- all_counties %>%
#     filter(STATEFP== "11")
# 
# # create empty list to save DC PM data into; (each list element is 1 df with one row, for the DC estimate that year.)
# so4_dc_list = list()
# 
# # count variable for year to name saved out df
# year = 2000
# ## Run loop for 2000 through 2017
# for (i in 1:18){
#   # read in ncdf as raster stack for bcs
#   so4_year = raster(paste("data/raw_components/so4/", so4_years[i], ".nc", sep = ""))
#   so4_year = stack(so4_year)
# 
# 
#   # read in ncdf as raster stack for total PM in 2001
#   pm_year = raster(paste("data/raw_components/total_pm/", pm_years[i], ".nc", sep = ""))
#   pm_year = stack(pm_year)
# 
#   # get the actual raster matrix for BC through multiplying percentage with total PM
#   so4_actual_year = (so4_year*pm_year)/100
#   mean(getValues(so4_actual_year), na.rm = T) # average county mean
# 
# # extract data for just DC (fips = 11)
#   so4_dc_list[[i]] <- raster::extract(so4_actual_year,
#                             DC_state_list,
#                             fun=mean, na.rm=TRUE, df=TRUE) %>% # specify function = mean to extract mean concentrations
#     mutate(year = year) %>%
#     dplyr::rename(FIPS = ID) %>%
#     dplyr::rename(so4 = layer) %>%
#     mutate(State = 11) %>%
#     mutate(FIPS = 11001)
#   so4_dc_list[[i]]$County <- "001"
#   year = year + 1
# 
# 
# }
# toc()
# so4_dc_list
# so4_dc = do.call(rbind, so4_dc_list)
# so4_dc
# 
# 
# 
# # PM25 for each county should be saved now in the previously empty list
# # (each list item = one state, each row = county within that state)
# write_csv(so4_dc,
#           paste("data/county_concentrations/so4/so4_", year, "_df.csv", sep = ""))
# year = year + 1
# 
# # read in ncdf as raster stack for example year: 2008
# setwd("data/raw_components")
# so4_2008 <- raster("so4_2008.nc")
# so4_2008 <- stack(so4_2008)
# 
# # Make empty list to store model data, iterate through 48 states to extract mean PM2.5 concentrations
# so4_2008_counties <-list()
# 
# # function to extract values from raster into counties shapefile
# extract_model <- function(list, nc, state_list, state_fips){
#    list[[i]] <- raster::extract(nc, state_list[[i]],
#                                 fun=mean, na.rm=TRUE, df=TRUE) #specify function = mean to extract mean concentrations
#    list[[i]]$County <- state_list[[i]]$COUNTYFP
#    list[[i]]$State <- state_fips[i]
#    list[[i]]$FIPS <- paste(list[[i]]$State,list[[i]]$County)
#    list[[i]]$FIPS <- str_replace_all(list[[i]]$FIPS, " ", "")
# }
# extract_model(so4_2008_counties,
#               so4_2008, state_list, state_fips) #state_list and state_fips are already a list and vector created above
# 
# # 8/18/20 try running single extraction for DC for 2008
# raster::extract(so4_2008, DC_state_list,fun=mean, na.rm=TRUE, df=TRUE)
# 
# # read in all model PM25 datasets in a loop
# model_PM25_raw <- list()
# model_pm25_stack <- list()
# model_names <- c("GWRwSPEC_PM25_NA_200001_200012-RH35.nc",
#                  "GWRwSPEC.HEI.ELEVandURB_PM25_NA_200101_200112-RH35.nc",
#                  "GWRwSPEC.HEI.ELEVandURB_PM25_NA_200201_200212-RH35.nc",
#                  "GWRwSPEC.HEI.ELEVandURB_PM25_NA_200301_200312-RH35.nc",
#                  "GWRwSPEC.HEI.ELEVandURB_PM25_NA_200401_200412-RH35.nc",
#                  "GWRwSPEC.HEI.ELEVandURB_PM25_NA_200501_200512-RH35.nc",
#                  "GWRwSPEC.HEI.ELEVandURB_PM25_NA_200601_200612-RH35.nc",
#                  "GWRwSPEC.HEI.ELEVandURB_PM25_NA_200701_200712-RH35.nc",
#                  "GWRwSPEC_PM25_NA_200801_200812-RH35.nc",
#                  "GWRwSPEC_PM25_NA_200901_200912-RH35.nc",
#                  "GWRwSPEC_PM25_NA_201001_201012-RH35.nc",
#                  "GWRwSPEC_PM25_NA_201101_201112-RH35.nc",
#                  "GWRwSPEC_PM25_NA_201201_201212-RH35.nc",
#                  "GWRwSPEC_PM25_NA_201301_201312-RH35.nc",
#                  "GWRwSPEC_PM25_NA_201401_201412-RH35.nc",
#                  "GWRwSPEC_PM25_NA_201501_201512-RH35.nc",
#                  "GWRwSPEC_PM25_NA_201601_201612-RH35.nc",
#                  "GWRwSPEC.HEI_PM25_NA_201701_201712-RH35.nc",
#                  "GWRwSPEC.HEI_PM25_NA_201801_201812-RH35.nc")
# model_names[1]
# setwd("/Volumes/Seagate Backup Plus Drive/Rotations/Ana/SHS_Air_Pollution/Data/PM25_model")
# year <- 2000
# for (i in 1:19){
#   model_yr <- model_names[i]
#   model_PM25_raw[[i]] <- raster(model_yr)
#   model_pm25_stack[[i]] <- stack(model_PM25_raw[[i]])
#   year <- year + 1
# }
# summary(model_pm25_stack)
# 
# # now try running all extractions for DC for all 19 study years
# DC_modelPM25 <- list()
# for (i in 1:19){
#   DC_modelPM25[[i]] <- raster::extract(model_pm25_stack[[i]], DC_state_list,fun=mean, na.rm=TRUE, df=TRUE)
# }
# DC_modelPM25[[19]]
# 
# # manipulate table to be the same format as other model PM25 data
# year <- 2000
# for (i in 1:19){
#   DC_modelPM25[[i]] <- DC_modelPM25[[i]] %>% dplyr::rename(FIPS = ID) %>%
#     mutate(State = 11) %>% mutate(FIPS = 11001) %>% mutate(Year = year)
#   year <- year + 1
# }
# 
# DC_modelPM25
```

```

