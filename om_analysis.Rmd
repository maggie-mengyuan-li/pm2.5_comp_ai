---
title: "Organic Matter (OM) Analysis"
author: "Maggie Li (ml4424)"
date: "9/28/2021"
output: html_document
---


```{r load libraries}
library(tidyverse)
library(janitor)
library(stats)
library(lme4)
library(raster)
library(sf)
library(usmap)
library(tictoc)
require(lmerTest)
```


## Aim 1: Compare county-level OM concentrations in 2000-2017 between AI and non-AI-populated counties

```{r read in file with county type defined}
ai_county_fips = read_csv("data/ai_counties.csv") 

nai_county_fips = read_csv("data/nai_counties.csv") 
```

## Extract county-level OM concentrations for 2000-2017

Data Source: Randall Martin's PM2.5 Model https://sites.wustl.edu/acag/datasets/surface-pm2-5/ 
Note: The raw data show the proportion of each component, so have to multiply it by the total PM concentration that year

For Vivian: The next 2 code chunks will take a while to run, especially looping through all years. If it's too much, you can run just one year or a few; the outputted datasets are already in the folders indicated by write_csv.

```{r}
# read in all counties shapefile
counties_shp <- "data/cb_2018_us_county_500k/cb_2018_us_county_500k.shp"
all_counties <- st_read(counties_shp, stringsAsFactors = FALSE)

# group all_counties by state fips, exclude territories and hawaii and alaska
all_counties <- all_counties %>% arrange(STATEFP) %>%
  filter(STATEFP != "02",
         STATEFP != "15")
unique(all_counties$STATEFP)
all_counties

# each state's fips code in a vector
state_fips <- fips(state.name, county = c())

# remove hawaii and alaska (FIPS are 15 and 02)
state_fips <- state_fips[!(state_fips %in% c("02","15"))]

# save each state's counties as separate sf's, in a list. the function below will iterate through all of these states.
state_list <- list(length(state_fips)) # create list of length 48
for (i in 1:length(state_fips)){
  state_list[[i]] <- all_counties %>%
    filter(STATEFP==state_fips[i])
}
# state_list # each list value = one state; includes polygon counties in that state; 48 states total
```

```{r run loop to read in and get all OM data 2000-2017}
# Loop from 2000-2017
om_years = c("om_2000", "om_2001", "om_2002", "om_2003", "om_2004", "om_2005",
             "om_2006", "om_2007", "om_2008", "om_2009",
             "om_2010", "om_2011", "om_2012", "om_2013",
             "om_2014", "om_2015", "om_2016", "om_2017")

pm_years = c("pm_2000", "pm_2001", "pm_2002", "pm_2003", "pm_2004", "pm_2005",
             "pm_2006", "pm_2007", "pm_2008", "pm_2009",
             "pm_2010", "pm_2011", "pm_2012", "pm_2013",
             "pm_2014", "pm_2015", "pm_2016", "pm_2017")


# Extract Data for DC
## create a df just DC all years, join with the existing components_year_df files in data/county_concentrations
DC_state_list <- all_counties %>%
    filter(STATEFP== "11")

# create empty list to save DC PM data into; (each list element is 1 df with one row, for the DC estimate that year.)
om_dc_list = list()

# count variable for year to name saved out df
year = 2000
## Run loop for 2000 through 2017
for (i in 1:18){
  # read in ncdf as raster stack for oms
  om_year = raster(paste("data/raw_components/om/", om_years[i], ".nc", sep = ""))
  om_year = stack(om_year)


  # read in ncdf as raster stack for total PM in 2001
  pm_year = raster(paste("data/raw_components/total_pm/", pm_years[i], ".nc", sep = ""))
  pm_year = stack(pm_year)

  # get the actual raster matrix for om through multiplying percentage with total PM
  om_actual_year = (om_year*pm_year)/100
  mean(getValues(om_actual_year), na.rm = T) # average county mean

# extract data for just DC (fips = 11)
  om_dc_list[[i]] <- raster::extract(om_actual_year,
                            DC_state_list,
                            fun=mean, na.rm=TRUE, df=TRUE) %>% # specify function = mean to extract mean concentrations
    mutate(year = year) %>%
    dplyr::rename(FIPS = ID) %>%
    dplyr::rename(om = layer) %>%
    mutate(State = 11) %>%
    mutate(FIPS = 11001)
  om_dc_list[[i]]$County <- "001"
  year = year + 1


}
toc()
om_dc_list
om_dc = do.call(rbind, om_dc_list)
om_dc

tic()

## count variable for year to name saved out df
year = 2000
## Run loop for 2000 through 2017
for (i in 1:18){
  # read in ncdf as raster stack for bcs
  om_year = raster(paste("data/raw_components/om/", om_years[i], ".nc", sep = ""))
  om_year = stack(om_year)


  # read in ncdf as raster stack for total PM
  pm_year = raster(paste("data/raw_components/total_pm/", pm_years[i], ".nc", sep = ""))
  pm_year = stack(pm_year)

  # get the actual raster matrix for BC through multiplying percentage with total PM
  om_actual_year = (om_year*pm_year)/100
  mean(getValues(om_actual_year), na.rm = T) # average county mean

  # create empty lists to save dfs into
  om_year_list = list()

  # write loop to extract county BC levels for 2001: this will take a while to run

  tic() # tictoc helps record the time it takes to run things in R
  for (i in 1:48){ # 48 states total FIPS = 01 (AL), 04 (AZ)
    om_year_list[[i]] <- raster::extract(om_actual_year,
                                         state_list[[i]],
                                         fun=mean, na.rm=TRUE, df=TRUE) #specify function = mean to extract mean concentrations
    om_year_list[[i]]$County <- state_list[[i]]$COUNTYFP
    om_year_list[[i]]$State <- state_fips[i]
    om_year_list[[i]]$FIPS <- paste(om_year_list[[i]]$State,
                                    om_year_list[[i]]$County)
    om_year_list[[i]]$FIPS <- str_replace_all(om_year_list[[i]]$FIPS, " ", "")}

  toc()

  # PM25 for each county should be saved now in the previously empty list
  # (each list item = one state, each row = county within that state)
  om_year_df = do.call(rbind, om_year_list)
  write_csv(om_year_df,
            paste("data/county_concentrations/om/om_", year, "_df.csv", sep = ""))
  year = year + 1

}
toc()
```

```{r combine all years into one df}
# Read in model data into list of length 19 (# study years)
om_allyrs <- list()
yr = 2000
for (i in 1:18){
  om_allyrs[[i]] <- read_csv(file = paste('data/county_concentrations/om/om_',
                                           yr, '_df.csv', sep = '')) %>%
    dplyr::rename(om = layer) %>% # rename "layer" column to specify component (OM)
    mutate(year = yr) # add column for year
  yr <- yr + 1
}

om_allyrs = do.call(rbind, om_allyrs) # join list into one df

summary(om_allyrs)
View(om_allyrs)
# write out for descriptive stats
write_csv(om_allyrs, "data/county_concentrations/allyrs/om_allyrs.csv")
```

```{r combine all years into one df}
# Read in model data into list of length 19 (# study years)
om_allyrs <- list()
yr = 2000
for (i in 1:18){
  om_allyrs[[i]] <- read_csv(file = paste('data/county_concentrations/om/om_',
                                           yr, '_df.csv', sep = '')) %>%
    dplyr::rename(om = layer) %>% # rename "layer" column to specify component (om)
    mutate(year = yr) # add column for year
  yr <- yr + 1
}

om_allyrs = do.call(rbind, om_allyrs) # join list into one df
om_allyrs
# UPDATE: join with DC data
om_allyrs = om_allyrs %>% dplyr::select(-ID)
om_allyrs = rbind(om_allyrs, om_dc)

summary(om_allyrs)
length(unique(om_allyrs$FIPS))

# write out Final joined dataset w/ 48 states + DC
write_csv(om_allyrs, "data/county_concentrations/allyrs/om_allyrs.csv")

```

## Assign OM estimates to AI vs non-AI counties

```{r join layer to AI county}
om_allyrs = read_csv("data/county_concentrations/allyrs/om_allyrs.csv")

all_county_types = rbind(ai_county_fips, nai_county_fips) %>% 
  rename(FIPS = County)

om_ai_join = left_join(om_allyrs, all_county_types) %>% 
  dplyr::select(FIPS, om, year, county_type) %>% 
  rename(County = FIPS) %>% 
  mutate(county_type = replace_na(county_type, 1)) ## county FIPS 46102 is showing up as an NA
om_ai_join
```

```{r join with covariates}
covariates = read_csv("data/covariates.csv")
summary(covariates)
om_ai_join = om_ai_join %>% 
  left_join(covariates)
om_ai_join

# Compare summaries in all counties, AI, non-AI
summary(om_ai_join)
summary(om_ai_join %>% filter(county_type == 1))
summary(om_ai_join %>% filter(county_type == 0))

# note higher median/mean levels in nAI counties (unadjusted)
```

```{r add columns for deciles}
# Split population density and hhincome into deciles for model
om_ai_join$popd_q <- cut(om_ai_join$pop_density, quantile(om_ai_join$pop_density, seq(0,1,0.1)), include.lowest = TRUE)
om_ai_join$hhinc_q <- cut(om_ai_join$hh_income, quantile(om_ai_join$hh_income, seq(0,1,0.1)), include.lowest = TRUE)
sum(table(om_ai_join$popd_q, exclude = NULL)) == dim(om_ai_join)[1]
om_ai_join

  # set referent for df
om_ai_join = om_ai_join %>% 
  mutate(county_type = factor(county_type)) %>% 
  mutate(county_type = relevel(county_type, ref="0"))

## Save out dataframe 
write_csv(om_ai_join,
          "data/county_concentrations/joined_dta/om_ai_join.csv")
```
## Statistical Models:

```{r Run lmer models}
# unadjusted
om_unadj = lmer(om ~ county_type + as.factor(year) +
                    (1|State/County),
                  data = om_ai_join)


## 95% CI: point estimate, LL, UL
paste(summary(om_unadj)$coefficients[2,1] %>% round(digits = 3),
      " (",
      round(summary(om_unadj)$coefficients[2,1] - 1.96*summary(om_unadj)$coefficients[2,2], digits = 3),
      ", ",
      round(summary(om_unadj)$coefficients[2,1] + 1.96*summary(om_unadj)$coefficients[2,2], digits = 3),
      ")", sep = "")

# additionally adjusted for popd
om_partial = lmer(om ~ county_type + as.factor(year) +
                  popd_q +
                  (1|State/County),
                data = om_ai_join)


## 95% CI: point estimate, LL, UL
paste(summary(om_partial)$coefficients[2,1] %>% round(digits = 3),
      " (",
      round(summary(om_partial)$coefficients[2,1] - 1.96*summary(om_partial)$coefficients[2,2], digits = 3),
      ", ",
      round(summary(om_partial)$coefficients[2,1] + 1.96*summary(om_partial)$coefficients[2,2], digits = 3),
      ")", sep = "")

# full adj model
om_full = lmer(om ~ county_type + as.factor(year) +
                    popd_q + 
                    hhinc_q +
                    (1|State/County),
                  data = om_ai_join, REML=FALSE)

## 95% CI: point estimate, LL, UL
paste(summary(om_full)$coefficients[2,1] %>% round(digits = 3),
      " (",
      round(summary(om_full)$coefficients[2,1] - 1.96*summary(om_full)$coefficients[2,2], digits = 3),
      ", ",
      round(summary(om_full)$coefficients[2,1] + 1.96*summary(om_full)$coefficients[2,2], digits = 3),
      ")", sep = "")
paste(summary(om_full)$coefficients[2,1] %>% round(digits = 2),
      " (",
      round(summary(om_full)$coefficients[2,1] - 1.96*summary(om_full)$coefficients[2,2], digits = 2),
      ", ",
      round(summary(om_full)$coefficients[2,1] + 1.96*summary(om_full)$coefficients[2,2], digits = 2),
      ")", sep = "")

# fully adjusted with interaction term for year
om_full_interx = lmer(om ~ county_type + as.factor(year) +
                    popd_q + 
                    hhinc_q +
                    county_type*as.factor(year) +
                    (1|State/County),
                  data = om_ai_join, REML=FALSE)
summary(om_full_interx)
```

```{r Save out results from main effects only model and interx effects model}

## save model output for main effects only model
saveRDS(om_full,
        file = "intermediate/model_outputs/main_results/om_main_effects.rds")
## save model output for main effects only model
saveRDS(om_full_interx,
        file = "intermediate/model_outputs/main_results/om_interx.rds")

```


```{r InterX plot from lmer}
# create vcov matrix of main effects and interX

# extract vcov matrix of the county type main effect and interaction effect for each categorical year
native_yr_vcov <- vcov(om_full_interx)[c(2,seq(38,54)), c(2,seq(38,54))]

# calculate all the variances for all the years i.e. var(x+y); should be 19 total entries --> var_vector
# var_vector should be the same repeating value (from the diagonal of the vcov matrix above)
var_vector = c()
for (i in 2:18){
  var_vector[1] <- native_yr_vcov[1,1]
  var_vector[i] <- native_yr_vcov[1,1] + native_yr_vcov[i,i] + 2 * native_yr_vcov[i,1]
}
var_vector
sd_vector <- sqrt(var_vector)
length(sd_vector)
#matrix with 19 cols for 19 years, three rows: one for effect estimate of total effect per year (total effect = main effect + interx effect), one for CI lower, one for CI upper
om_decline <- data.frame()
om_decline[1,1] <- summary(om_full_interx)$coefficients[2,1]
om_decline[1,2] <- summary(om_full_interx)$coefficients[2,1] - 1.96*sd_vector[1]
om_decline[1,3] <- summary(om_full_interx)$coefficients[2,1] + 1.96*sd_vector[1]

# fill in matrix thru loop for every following year
yr_ct <- 38
for (i in 2:18){
  om_decline[i,1] <- summary(om_full_interx)$coefficients[2,1]+
    summary(om_full_interx)$coefficients[yr_ct,1]
  
  om_decline[i,2] <- summary(om_full_interx)$coefficients[2,1]+
    summary(om_full_interx)$coefficients[yr_ct,1] - 1.96*sd_vector[i]
  
  om_decline[i,3] <- summary(om_full_interx)$coefficients[2,1]+
    summary(om_full_interx)$coefficients[yr_ct,1] + 1.96*sd_vector[i]
  yr_ct <- yr_ct + 1
}

om_decline

colnames(om_decline) <- c('estimate', 'cl_lower', 'cl_upper') # set col names

om_decline$year <- seq(2000, 2017) # column for year


#PLOT OF TOTAL EFFECT OF NATIVE OVER TIME; updated 12/14 to be consistent with Figure 3 layout (slanted x-axis label)
om_interx_plot <- ggplot() + 
  theme_linedraw() + 
  geom_line(data = om_decline,
            aes(x=year, y = estimate)) +
  geom_line(data=om_decline,
            aes(x=year, y=cl_lower), linetype = "dashed") +
  geom_line(data=om_decline,
            aes(x=year, y=cl_upper), linetype = "dashed") +
  ylim(-0.6, 0.6) +
  labs(x = "Year",
       y = expression(paste("Mean Difference in OM (", mu, "g/", m^3, ")")),
       fill = "County Type") +
  theme(plot.title = element_text(size = 18),
        axis.title = element_text(size = 16),
        axis.text = element_text(size = 12),
        axis.title.x=element_blank(),
        panel.background = element_rect(fill='transparent'), 
        plot.background = element_rect(fill='transparent', color=NA)) +
  scale_x_continuous(breaks = seq(2000,2017,1), expand = c(0, 0)) +
  guides(x =  guide_axis(angle = 45)) +
  geom_hline(yintercept=0, linetype="solid", color = "red")
om_interx_plot

ggsave("figures/effectmod_plots/main/om.png", width = 6, height = 4, dpi = 300,
       bg='transparent')

```

#### Smooth interX model with time
```{r 10 knots}
# # using splines
# library(splines)
# library(mgcv)
# library(gamm4)
# om_full_interx_smooth = gamm4(om ~ s(year, by = county_type, k = 10) + # specify k knots
#                     county_type + 
#                       popd_q + 
#                     hhinc_q,
#                     random = ~(1|State/County),
#                   data =  om_ai_join)
# summary(om_full_interx_smooth)
# summary(om_full_interx_smooth$gam)
# 
# plot(om_full_interx_smooth$gam, pages = 1)
# 
# 
# 
# # empty df for predicted values from gam
# pdat = expand.grid(year = seq(2000, 2017, length = 170),
#                     county_type = c(0, 1),
#                    popd_q = "(160,382]",
#                    hhinc_q = "(4.45e+04,4.72e+04]")
# 
# 
# # add in predicted values from gam
# xp <- predict(om_full_interx_smooth$gam, newdata = pdat, type = 'lpmatrix')
# xp
# 
# ## which cols of xp relate to splines of interest?
# c1 <- grepl('0', colnames(xp))
# c2 <- grepl('1', colnames(xp))
# ## which rows of xp relate to sites of interest?
# r1 <- with(pdat, county_type == '0')
# r2 <- with(pdat, county_type == '1')
# 
# ## difference rows of xp for data from comparison
# X <- xp[r1, ] - xp[r2, ]
# ## zero out cols of X related to splines for other lochs
# X[, ! (c1 | c2)] <- 0
# ## zero out the parametric cols
# X[, !grepl('^s\\(', colnames(xp))] <- 0
# #extract betas
# dif <- X %*% coef(om_full_interx_smooth$gam)
# #extract se
# se <- sqrt(rowSums((X %*% vcov(om_full_interx_smooth$gam)) * X))
# #calculate CI
# crit <- qt(.975, df.residual(om_full_interx_smooth$gam))
# upr <- dif + (crit * se)
# lwr <- dif - (crit * se)
# 
# # function to do it all in 1
# smooth_diff <- function(model, newdata, f1, f2, var, alpha = 0.05,
#                         unconditional = FALSE) {
#     xp <- predict(model, newdata = newdata, type = 'lpmatrix')
#     c1 <- grepl(f1, colnames(xp))
#     c2 <- grepl(f2, colnames(xp))
#     r1 <- newdata[[var]] == f1
#     r2 <- newdata[[var]] == f2
#     ## difference rows of xp for data from comparison
#     X <- xp[r1, ] - xp[r2, ]
#     ## zero out cols of X related to splines for other lochs
#     X[, ! (c1 | c2)] <- 0
#     ## zero out the parametric cols
#     X[, !grepl('^s\\(', colnames(xp))] <- 0
#     dif <- X %*% coef(model)
#     se <- sqrt(rowSums((X %*% vcov(model, unconditional = unconditional)) * X))
#     crit <- qt(alpha/2, df.residual(model), lower.tail = FALSE)
#     upr <- dif + (crit * se)
#     lwr <- dif - (crit * se)
#     data.frame(pair = paste(f1, f2, sep = '-'),
#                diff = dif,
#                se = se,
#                upper = upr,
#                lower = lwr)
# }
# 
# comp1 <- smooth_diff(om_full_interx_smooth$gam, pdat, '1', '0', 'county_type')
# comp <- cbind(year = seq(2000, 2017, length = 170),
#               rbind(comp1))
# 
# comp
# 
# # plot smoothed difference
# ggplot(comp, aes(x = year, y = diff, group = pair)) +
#     geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
#     geom_line() +
#   labs(x = "Year",
#        y = expression(paste("Mean Difference in OM (", mu, "g/", m^3, ")")),
#        fill = "County Type") +
#   theme(plot.title = element_text(size = 18),
#         axis.title = element_text(size = 16),
#         axis.text = element_text(size = 12),
#         axis.title.x=element_blank()) +
#   scale_x_continuous(breaks = seq(2000,2017,1), expand = c(0, 0)) +
#   guides(x =  guide_axis(angle = 45)) +
#   geom_hline(yintercept=0, linetype="solid", color = "red") + 
#   ylim(-0.3, 0.3) +
#   theme_linedraw()
# ggsave("figures/effectmod_plots/om/om_smooth_k10.png", width = 6, height = 4, dpi = 300)
# 
# # compare with original glmm
# om_interx_plot
```

```{r 7 knots}
# # using splines
# library(splines)
# library(mgcv)
# library(gamm4)
# om_full_interx_smooth = gamm4(om ~ s(year, by = county_type, k = 7) + # specify k knots
#                     county_type + 
#                       popd_q + 
#                     hhinc_q,
#                     random = ~(1|State/County),
#                   data =  om_ai_join)
# summary(om_full_interx_smooth)
# summary(om_full_interx_smooth$gam)
# 
# plot(om_full_interx_smooth$gam, pages = 1)
# 
# 
# 
# # empty df for predicted values from gam
# pdat = expand.grid(year = seq(2000, 2017, length = 170),
#                     county_type = c(0, 1),
#                    popd_q = "(160,382]",
#                    hhinc_q = "(4.45e+04,4.72e+04]")
# 
# 
# # add in predicted values from gam
# xp <- predict(om_full_interx_smooth$gam, newdata = pdat, type = 'lpmatrix')
# xp
# 
# ## which cols of xp relate to splines of interest?
# c1 <- grepl('0', colnames(xp))
# c2 <- grepl('1', colnames(xp))
# ## which rows of xp relate to sites of interest?
# r1 <- with(pdat, county_type == '0')
# r2 <- with(pdat, county_type == '1')
# 
# ## difference rows of xp for data from comparison
# X <- xp[r1, ] - xp[r2, ]
# ## zero out cols of X related to splines for other lochs
# X[, ! (c1 | c2)] <- 0
# ## zero out the parametric cols
# X[, !grepl('^s\\(', colnames(xp))] <- 0
# #extract betas
# dif <- X %*% coef(om_full_interx_smooth$gam)
# #extract se
# se <- sqrt(rowSums((X %*% vcov(om_full_interx_smooth$gam)) * X))
# #calculate CI
# crit <- qt(.975, df.residual(om_full_interx_smooth$gam))
# upr <- dif + (crit * se)
# lwr <- dif - (crit * se)
# 
# # function to do it all in 1
# smooth_diff <- function(model, newdata, f1, f2, var, alpha = 0.05,
#                         unconditional = FALSE) {
#     xp <- predict(model, newdata = newdata, type = 'lpmatrix')
#     c1 <- grepl(f1, colnames(xp))
#     c2 <- grepl(f2, colnames(xp))
#     r1 <- newdata[[var]] == f1
#     r2 <- newdata[[var]] == f2
#     ## difference rows of xp for data from comparison
#     X <- xp[r1, ] - xp[r2, ]
#     ## zero out cols of X related to splines for other lochs
#     X[, ! (c1 | c2)] <- 0
#     ## zero out the parametric cols
#     X[, !grepl('^s\\(', colnames(xp))] <- 0
#     dif <- X %*% coef(model)
#     se <- sqrt(rowSums((X %*% vcov(model, unconditional = unconditional)) * X))
#     crit <- qt(alpha/2, df.residual(model), lower.tail = FALSE)
#     upr <- dif + (crit * se)
#     lwr <- dif - (crit * se)
#     data.frame(pair = paste(f1, f2, sep = '-'),
#                diff = dif,
#                se = se,
#                upper = upr,
#                lower = lwr)
# }
# 
# comp1 <- smooth_diff(om_full_interx_smooth$gam, pdat, '1', '0', 'county_type')
# comp <- cbind(year = seq(2000, 2017, length = 170),
#               rbind(comp1))
# 
# comp
# 
# # plot smoothed difference
# ggplot(comp, aes(x = year, y = diff, group = pair)) +
#     geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
#     geom_line() +
#   labs(x = "Year",
#        y = expression(paste("Mean Difference in OM (", mu, "g/", m^3, ")")),
#        fill = "County Type") +
#   theme(plot.title = element_text(size = 18),
#         axis.title = element_text(size = 16),
#         axis.text = element_text(size = 12),
#         axis.title.x=element_blank()) +
#   scale_x_continuous(breaks = seq(2000,2017,1), expand = c(0, 0)) +
#   guides(x =  guide_axis(angle = 45)) +
#   geom_hline(yintercept=0, linetype="solid", color = "red") + 
#   ylim(-0.3, 0.3) +
#   theme_linedraw()
# ggsave("figures/effectmod_plots/om/om_smooth_k7.png", width = 6, height = 4, dpi = 300)
# 
# # compare with original glmm
# om_interx_plot
```


```{r 5 knots}
# # using splines
# library(splines)
# library(mgcv)
# library(gamm4)
# om_full_interx_smooth = gamm4(om ~ s(year, by = county_type, k = 5) + # specify k knots
#                     county_type + 
#                       popd_q + 
#                     hhinc_q,
#                     random = ~(1|State/County),
#                   data =  om_ai_join)
# summary(om_full_interx_smooth)
# summary(om_full_interx_smooth$gam)
# 
# plot(om_full_interx_smooth$gam, pages = 1)
# 
# 
# 
# # empty df for predicted values from gam
# pdat = expand.grid(year = seq(2000, 2017, length = 170),
#                     county_type = c(0, 1),
#                    popd_q = "(160,382]",
#                    hhinc_q = "(4.45e+04,4.72e+04]")
# 
# 
# # add in predicted values from gam
# xp <- predict(om_full_interx_smooth$gam, newdata = pdat, type = 'lpmatrix')
# xp
# 
# ## which cols of xp relate to splines of interest?
# c1 <- grepl('0', colnames(xp))
# c2 <- grepl('1', colnames(xp))
# ## which rows of xp relate to sites of interest?
# r1 <- with(pdat, county_type == '0')
# r2 <- with(pdat, county_type == '1')
# 
# ## difference rows of xp for data from comparison
# X <- xp[r1, ] - xp[r2, ]
# ## zero out cols of X related to splines for other lochs
# X[, ! (c1 | c2)] <- 0
# ## zero out the parametric cols
# X[, !grepl('^s\\(', colnames(xp))] <- 0
# #extract betas
# dif <- X %*% coef(om_full_interx_smooth$gam)
# #extract se
# se <- sqrt(rowSums((X %*% vcov(om_full_interx_smooth$gam)) * X))
# #calculate CI
# crit <- qt(.975, df.residual(om_full_interx_smooth$gam))
# upr <- dif + (crit * se)
# lwr <- dif - (crit * se)
# 
# # function to do it all in 1
# smooth_diff <- function(model, newdata, f1, f2, var, alpha = 0.05,
#                         unconditional = FALSE) {
#     xp <- predict(model, newdata = newdata, type = 'lpmatrix')
#     c1 <- grepl(f1, colnames(xp))
#     c2 <- grepl(f2, colnames(xp))
#     r1 <- newdata[[var]] == f1
#     r2 <- newdata[[var]] == f2
#     ## difference rows of xp for data from comparison
#     X <- xp[r1, ] - xp[r2, ]
#     ## zero out cols of X related to splines for other lochs
#     X[, ! (c1 | c2)] <- 0
#     ## zero out the parametric cols
#     X[, !grepl('^s\\(', colnames(xp))] <- 0
#     dif <- X %*% coef(model)
#     se <- sqrt(rowSums((X %*% vcov(model, unconditional = unconditional)) * X))
#     crit <- qt(alpha/2, df.residual(model), lower.tail = FALSE)
#     upr <- dif + (crit * se)
#     lwr <- dif - (crit * se)
#     data.frame(pair = paste(f1, f2, sep = '-'),
#                diff = dif,
#                se = se,
#                upper = upr,
#                lower = lwr)
# }
# 
# comp1 <- smooth_diff(om_full_interx_smooth$gam, pdat, '1', '0', 'county_type')
# comp <- cbind(year = seq(2000, 2017, length = 170),
#               rbind(comp1))
# 
# comp
# 
# # plot smoothed difference
# ggplot(comp, aes(x = year, y = diff, group = pair)) +
#     geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
#     geom_line() +
#   labs(x = "Year",
#        y = expression(paste("Mean Difference in OM (", mu, "g/", m^3, ")")),
#        fill = "County Type") +
#   theme(plot.title = element_text(size = 18),
#         axis.title = element_text(size = 16),
#         axis.text = element_text(size = 12),
#         axis.title.x=element_blank()) +
#   scale_x_continuous(breaks = seq(2000,2017,1), expand = c(0, 0)) +
#   guides(x =  guide_axis(angle = 45)) +
#   geom_hline(yintercept=0, linetype="solid", color = "red") + 
#   ylim(-0.3, 0.3) +
#   theme_linedraw()
# ggsave("figures/effectmod_plots/om/om_smooth_k5.png", width = 6, height = 4, dpi = 300)
# 
# # compare with original glmm
# om_interx_plot
```
